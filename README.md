# ğŸ’€ Age of Death Predictor

This project predicts the **age of death** for patients who experienced **heart failure**, using clinical features and various regression models. It includes data exploration, training, evaluation, and prediction capabilities with visualizations.

---

## ğŸ“ Project Structure
```
death-age-predictor/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ heart_failure_clinical_records.csv
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ best_model.joblib
â”‚   â”œâ”€â”€ best_model_meta.json
â”‚   â””â”€â”€ *.html  (visualizations)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_and_test.py
â”‚   â”œâ”€â”€ predictor.py
â”‚   â””â”€â”€ explore_data_app.py
```

# ğŸ§° Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```

# ğŸ“Š Data Source
Input file: data/heart_failure_clinical_records.csv
Download from: [Kaggle â€“ Heart Failure Clinical Records](https://www.kaggle.com/datasets/aadarshvelu/heart-failure-prediction-clinical-records)
It contains clinical features of patients, with DEATH_EVENT = 1 indicating
the patient died during follow-up and age at death. The time column represents the duration (in days) until the follow-up, with a maximum value of 285 days.
Given this, we assume that for patients with DEATH_EVENT = 1, the age column reflects their age at death.


# ğŸš€ Scripts Overview
## 1. `explore_data_app.py`

An interactive **Streamlit** web app for exploring the clinical data of deceased heart failure patients.

### ğŸ” Features
- Summary statistics
- Condition prevalence (e.g. anaemia, diabetes, high blood pressure, smoking)
- Feature distributions (e.g. age, ejection fraction)
- Correlation heatmap of numeric features

### â–¶ï¸ How to Launch
```bash
python src/train_and_test.py
```
or
```bash
streamlit run src/explore_data_app.py
```

## 2. `train_and_test.py`

Trains multiple regression models to predict the **age of death** for patients who died from heart failure.

### ğŸ“‹ Key Steps

- Filters only deceased patients (`DEATH_EVENT == 1`)
- Trains multiple models:
  - Linear Regression, Ridge, Lasso, ElasticNet
  - Decision Trees, Random Forest, Gradient Boosting
  - SVR, XGBoost, KNN
- Evaluates each using multiple random seeds
- Ranks models based on a user-defined metric: `mse`, `mae`, or `r2`
- Saves the **best model** and its training metadata

### ğŸ“ˆ Output Artifacts

- Learning curve plots
- Prediction vs actual scatter plots
- Feature importance visualizations (if available)
- `output/best_model.joblib` and `output/best_model_meta.json`

### â–¶ï¸ How to Run

```bash
python src/train_and_test.py
```

## 3. `predictor.py`

Uses the trained model to make predictions on **all patients**, including 
those who survived (for the time being ğŸ˜ˆ) 

### ğŸ” What it does:
- Loads the **trained model** and its **training metadata** (including the features used).
- Applies the **same preprocessing** used during training.
- Predicts the **age of death** for all patients in the dataset.
- Saves the **predictions to CSV**.
- Generates an **interactive Plotly chart**:
  - ğŸ”´ **Red**: Predicted age of death (all patients)
  - ğŸŸ¢ **Green**: True age (for patients who died)

### â–¶ï¸ Run it with:

```bash
python src/predictor.py
```


# ğŸ” Overfitting

This project includes several strategies to detect and reduce overfitting:

### âœ… Multiple Random Seeds
- Models are trained and evaluated using several random seeds: `[0, 42, 77, 123, 999]`.
- Results (MSE, MAE, RÂ²) are aggregated across seeds.
- Low variation between runs indicates stable model performance.

### ğŸ“‰ Learning Curve Plots
- Learning curves compare training and validation errors across increasing dataset sizes.
- In this project, validation error is typically ~15% higher than training error.
- This gap suggests **moderate overfitting**, understandable given the small amount of samples ~1500, but within an acceptable range.

### ğŸ“Š Feature Importance Visualization
- Tree-based models (e.g. Random Forest, XGBoost) provide interpretable feature importances.
- Helps identify:
  - Over-reliance on a few features
  - Potential for dimensionality reduction or regularization
